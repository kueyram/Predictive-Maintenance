{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c70b8a0",
   "metadata": {},
   "source": [
    "Optimizing Equipment Performance through Predictive Maintenance Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7266e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e09125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07458ee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/kueyram/Predictive-Maintenance/main/Data/machine_failure.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reading the dataset into pandas\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pred_maintenance_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Printing the columns of the dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pred_maintenance_df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    719\u001b[0m     path_or_buf,\n\u001b[0;32m    720\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    721\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    722\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    723\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    724\u001b[0m )\n\u001b[0;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Eyram\\anaconda3\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "url = 'https://raw.githubusercontent.com/kueyram/Predictive-Maintenance/main/Data/machine_failure.csv'\n",
    "\n",
    "# Reading the dataset into pandas\n",
    "pred_maintenance_df = pd.read_csv(url, sep=',')\n",
    "\n",
    "# Printing the columns of the dataset\n",
    "pred_maintenance_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the dataset and check its structure and contents\n",
    "pred_maintenance_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c949334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_maintenance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing 5 random columns from the dataset\n",
    "pred_maintenance_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check for duplicates\n",
    "\n",
    "# Checking for duplicates across all columns\n",
    "duplicates = pred_maintenance_df[pred_maintenance_df.duplicated()]\n",
    "\n",
    "# Checking if duplicates dataframe is empty\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicates found.\")\n",
    "else:\n",
    "    print(\"Duplicates found:\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65292b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "missing_values = pred_maintenance_df.isnull().any()\n",
    "\n",
    "# Checking if any column contains NaN\n",
    "if missing_values.any():\n",
    "    print(\"NaN values found in the following columns:\")\n",
    "    print(missing_values[missing_values].index.tolist())\n",
    "else:\n",
    "    print(\"No missing values found in the Dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4460c6a",
   "metadata": {},
   "source": [
    "Creating visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3aba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the columns we need for the chart\n",
    "air_temperature = pred_maintenance_df['Air temperature [K]']\n",
    "process_temperature = pred_maintenance_df['Process temperature [K]']\n",
    "\n",
    "# Line chart\n",
    "plt.plot(air_temperature, process_temperature, marker='o')\n",
    "plt.xlabel('Air Temperature (K)')\n",
    "plt.ylabel('Process Temperature (K)')\n",
    "plt.title('Air Temperature vs. Process Temperature')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208fa22",
   "metadata": {},
   "source": [
    "The chart shows that there is a positive correlation between the air temperature and the process temperature. We can conclude that environmental factors have impacts on the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the histogram for Process Temperature\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(pred_maintenance_df['Process temperature [K]'], bins=30, kde=True, color='orange')\n",
    "plt.xlabel('Process Temperature [K]')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Process Temperature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786ede7",
   "metadata": {},
   "source": [
    "The process Temperature is normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns not needed\n",
    "# To predict maintenance, we do not need Product ID. Therefore, we are dropping the columns\n",
    "\n",
    "# Removing the 'Product ID' column\n",
    "pred_maintenance_df.drop(['Product ID'], axis=1, inplace=True)\n",
    "\n",
    "# Print list of columns\n",
    "pred_maintenance_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535dc320",
   "metadata": {},
   "source": [
    "Feature transformation: We will convert Air temperature and Process temperature from Kelvin to Fahrenheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert temperature from kelvin to Fahrenheit\n",
    "def kelvin_to_f(temperature_kelvin):\n",
    "    temperature_f = ((temperature_kelvin-273.15)*9/5)+32\n",
    "    return temperature_f\n",
    "\n",
    "# Let's create a new columns called Air temperature [F]\n",
    "pred_maintenance_df['Air temperature [F]'] = pred_maintenance_df['Air temperature [K]'].apply(kelvin_to_f)\n",
    "\n",
    "# Let's create a new columns called Process temperature [F]\n",
    "pred_maintenance_df['Process temperature [F]'] = pred_maintenance_df['Process temperature [K]'].apply(kelvin_to_f)\n",
    "\n",
    "# Let's now drop the columns 'Air temperature [K]' and Process temperature [K]\n",
    "pred_maintenance_df.drop(['Air temperature [K]', 'Process temperature [K]'], axis=1, inplace=True)\n",
    "\n",
    "# Printing the first 5 rows of the dataset\n",
    "pred_maintenance_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1371c",
   "metadata": {},
   "source": [
    "Scaling the numerical columns using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Columns to scale\n",
    "numeric_columns = ['Process temperature [F]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Air temperature [F]']\n",
    "\n",
    "# Fitting the scaler\n",
    "scaler.fit(pred_maintenance_df[numeric_columns])\n",
    "\n",
    "# Transforming the columns usinf the fitted scaler\n",
    "pred_maintenance_df[numeric_columns] = scaler.transform(pred_maintenance_df[numeric_columns])\n",
    "\n",
    "# Printing the first 5 rows of the dataset\n",
    "pred_maintenance_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fbb55",
   "metadata": {},
   "source": [
    "Creating new features:\n",
    "- Power consumption: Energy used by equipment (rotational speed * torque)\n",
    "- Temperature difference: Temperature variation between air temperature and process temperature\n",
    "- Temperature ratio: Relationship between air temperature and process temperature   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power consumption\n",
    "pred_maintenance_df['Power consumption [W]'] = pred_maintenance_df['Rotational speed [rpm]'] * pred_maintenance_df['Torque [Nm]']\n",
    "\n",
    "# Difference between temperatures\n",
    "pred_maintenance_df['Temperature difference [F]'] = pred_maintenance_df['Air temperature [F]'] - pred_maintenance_df['Process temperature [F]']\n",
    "\n",
    "# Ratio of temperatures\n",
    "pred_maintenance_df['Temperature ratio'] = pred_maintenance_df['Air temperature [F]'] / pred_maintenance_df['Process temperature [F]']\n",
    "\n",
    "# Printing the first 5 rows of the dataset\n",
    "pred_maintenance_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing_values = pred_maintenance_df.isnull()\n",
    "total_missing_per_column = missing_values.sum()\n",
    "print(total_missing_per_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f7f29",
   "metadata": {},
   "source": [
    "The columns Temperature ration has 2 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding rows with missing values\n",
    "rows_with_missing_values = pred_maintenance_df[pred_maintenance_df.isnull().any(axis=1)]\n",
    "\n",
    "# Printing rows with missing values\n",
    "rows_with_missing_values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62e1eea0",
   "metadata": {},
   "source": [
    "The values are missing because we are dividing Air temperature by Process temperature which is 0 (Error in dividing by 0)\n",
    "We are going to replace the missing values with 0 giving the fact that both Air temperature and Process temperature are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the  NaN values with 0\n",
    "pred_maintenance_df['Temperature ratio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45c992",
   "metadata": {},
   "source": [
    "##### Creating dummy variables\n",
    "The columns Type is a categorical columns and has different values. We will create dummy variables for the Type column. This will help the model to understand the categorical relationship between the product Type and potential failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a6a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for Type\n",
    "dummy_variables = pd.get_dummies(pred_maintenance_df['Type'], prefix='Type')\n",
    "\n",
    "# Let's concatenate the dummy variables with the dataset\n",
    "predictive_maintenance_with_dummies = pd.concat([pred_maintenance_df, dummy_variables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first few rows of the new dataset\n",
    "predictive_maintenance_with_dummies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_maintenance_with_dummies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7658d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "plt.figure(figsize=(14, 10))\n",
    "features = ['Process temperature [F]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Air temperature [F]']\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(pred_maintenance_df[feature], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561222b",
   "metadata": {},
   "source": [
    "These graphs shows that the key features are relatively normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533597e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset to exclude \"No Failure\"\n",
    "filtered_df = pred_maintenance_df[pred_maintenance_df['Failure Type'] != 'No Failure']\n",
    "\n",
    "# Creating a horizontal bar chart for the filtered data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y='Failure Type', data=filtered_df, palette='viridis')\n",
    "plt.title('Distribution of failure types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Failure Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9493b04",
   "metadata": {},
   "source": [
    "The graph shows that Power and heat Dissipation are the main causes of equipment failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d6c4a",
   "metadata": {},
   "source": [
    "#### Creating the models\n",
    "We are going to build a two-stage model. We will first predict wether or not a failure will occure and then if a failure occurs, we will predict the type\n",
    "Let's add a new column called Failure. Its value will be 0 if No Failure and 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a3b67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_maintenance_df['Failure'] = pred_maintenance_df['Failure Type'].apply(lambda x: 0 if x == 'No Failure' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f538fb",
   "metadata": {},
   "source": [
    "We want to be able to predict if a failure will occur and what type of failure. We will use a combination of Logistic Regression for binary classification (failure/non-failure) and Random Forest for multiclass classification (failure types). Logistic regression will help us with the probability of failure and the Random Forest will help with predicting the type of failures"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b499075e",
   "metadata": {},
   "source": [
    "We are going to split the data into trainning and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab8c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the target columns before creating the dummy variables\n",
    "pred_maintenance_df_dropped = pred_maintenance_df.drop(columns=['Failure Type', 'Failure'])\n",
    "\n",
    "# Let's convert categorical variables to dummy variables\n",
    "pred_maintenance_df_converted = pd.get_dummies(pred_maintenance_df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f35658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_maintenance_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc9c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "features = pred_maintenance_df_converted\n",
    "target = pred_maintenance_df['Failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c6a5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c520cbf",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize and train a logistic regression model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "logreg_model.fit(features_train, target_train)\n",
    "\n",
    "# Let's predict and evaluate our model\n",
    "logreg_predictions = logreg_model.predict(features_test)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(target_test, logreg_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the accuracy\n",
    "accuracy = accuracy_score(target_test, logreg_predictions)\n",
    "# Printing the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(target_test, logreg_predictions, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(target_test, logreg_predictions, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "#F1-Score\n",
    "f1 = f1_score(target_test, logreg_predictions, average='weighted')\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7ebae",
   "metadata": {},
   "source": [
    "Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87dc67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the target for the multiclass classification\n",
    "target_multiclass = pred_maintenance_df['Failure Type']\n",
    "\n",
    "# SPlitting the data\n",
    "features_train_mc, features_test_mc, target_train_mc, target_test_mc = train_test_split(features, target_multiclass, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train and evaluate a random forest model RFM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initilization\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf_model.fit(features_train_mc, target_train_mc)\n",
    "\n",
    "# Prediction and evaluating the model\n",
    "rf_predictions = rf_model.predict(features_test_mc)\n",
    "print(\"Random Forest Model\")\n",
    "print(classification_report(target_test_mc, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3468d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Creating a dataframe for the feature importances\n",
    "feature_names = features.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of the feature importance\n",
    "importance_df_no_target = importance_df[importance_df['Feature'] != 'Target']\n",
    "\n",
    "# Plotting the filtered feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df_no_target, palette='viridis')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with the Random Forest model\n",
    "rf_predictions = rf_model.predict(features_test_mc)\n",
    "\n",
    "# Calculate Accuracy\n",
    "rf_accuracy = accuracy_score(target_test_mc, rf_predictions)\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Precision\n",
    "rf_precision = precision_score(target_test_mc, rf_predictions, average='macro')\n",
    "print(\"Precision:\", rf_precision)\n",
    "\n",
    "# Recall\n",
    "rf_recall = recall_score(target_test_mc, rf_predictions, average='macro')\n",
    "print(\"Recall:\", rf_recall)\n",
    "\n",
    "# F1-score\n",
    "rf_f1 = f1_score(target_test_mc, rf_predictions, average='macro')\n",
    "print(\"F1-Score:\", rf_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
